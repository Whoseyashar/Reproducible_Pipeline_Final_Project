# Reproducible_Pipeline_Final_Project

**University of Luxembourg**  
**Yashar Movahedi**

This project implements a reproducible pipeline for cleaning, analyzing, and visualizing a dataset from the Machine Learning Advanced course. It includes all necessary files, dependencies, and clear instructions to execute the pipeline efficiently.

For this project, we use the dataset on Amazon Sales Data ðŸ›’ðŸ“ˆ, which is included in this repository under the data/ directory as amazon.csv. This project is developed to demonstrate a Reproducible Pipeline for data cleaning, analysis, and visualization. The pipeline is implemented in Python and includes all necessary dependencies and documentation to execute the workflow efficiently.

For more information on the dataset and project, please consult the Project Documentation(https://www.kaggle.com/code/mehakiftikhar/amazon-sales-dataset-eda) You can also explore the dataset by cloning the repository and running the main Python script or exploring the optional Jupyter Notebook provided in the data/ folder.

To get started, follow the instructions in the Execution Guide to install dependencies and run the pipeline seamlessly:
---

## Project Overview
**Objective:**  
Create a reproducible pipeline for data processing and visualization.

**Dataset:**  
Amazon sales dataset located in the `data/` directory as `amazon.csv`.

**Outputs:**  
Cleaned data and visualizations are stored in the `outputs/` directory:
- Cleaned Dataset: `outputs/cleaned_amazon.csv`
- Visualization: `outputs/top_10_product_ids.png`

---

## Repository Structure

```plaintext
Reproducible_Pipeline_Final_Project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ amazon.csv                 # Raw dataset
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ cleaned_amazon.csv         # Cleaned dataset
â”‚   â”œâ”€â”€ top_10_product_ids.png     # Visualization of top product IDs
â”œâ”€â”€ WS2_FINAL_Project.py           # Main Python script
â”œâ”€â”€ Dockerfile                     # Docker configuration (optional)
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # Project documentation

```
## Instructions to Execute the Pipeline

1. Clone the Repository
  ```
git clone https://github.com/whoseyashar/Reproducible_Pipeline_Final_Project.git
cd Reproducible_Pipeline_Final_Project
 ```
2. Set Up Environment
```
python -m venv env
source env/bin/activate    # For Linux/MacOS
env\Scripts\activate       # For Windows
```
3. Install Dependencies
```
pip install -r requirements.txt
```
4. Run the Pipeline
```
python WS2_FINAL_Project.py

Outputs:

    The cleaned dataset will be saved as outputs/cleaned_amazon.csv.
    The visualization plot will be saved as outputs/top_10_product_ids.png.

## Instructions to Run Locally
1. Clone the Repository

git clone https://github.com/whoseyashar/Reproducible_Pipeline_Final_Project.git
cd Reproducible_Pipeline_Final_Project

2. Set Up Environment

 Ensure Python (version â‰¥3.9) is installed.
 Create a virtual environment:

    python -m venv env
    source env/bin/activate    # For Linux/MacOS
    env\Scripts\activate       # For Windows

3. Install Dependencies

pip install -r requirements.txt

4. Run the Pipeline

python WS2_FINAL_Project.py

 Outputs:

    The cleaned dataset will be saved in outputs/cleaned_amazon.csv.
    The visualization plot will be saved in outputs/top_10_product_ids.png.
```
## Optional:
we can Running with Docker

To run the pipeline in a Docker container:
1. Build the Docker Image

docker build -t reproducible_pipeline .

2. Run the Docker Container

docker run --rm -v "$(pwd):/app" reproducible_pipeline

## Key Features of the Pipeline
Data Loading: Ensures the dataset is loaded correctly or provides an error message if not found.
Data Cleaning: Handles missing values and removes unnecessary columns or rows.
Visualization: Creates a bar chart of the top 10 most frequent product IDs.
Reproducibility: Can be executed in any environment using provided dependencies or Docker.

## Future Work
Enhance visualizations with interactive plots.
Add automated tests for data validation.
Host pipeline as a web service using frameworks like Flask or FastAPI.



